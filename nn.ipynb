{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ' + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n = 12\n",
    "\n",
    "x_file_path = 'Datasets/kryptonite-' + str(n) + '-X.npy'\n",
    "y_file_path = 'Datasets/kryptonite-' + str(n) + '-y.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count: 24000\n"
     ]
    }
   ],
   "source": [
    "x_raw = torch.tensor(np.load(x_file_path), dtype=torch.float32)\n",
    "y_raw = torch.tensor(np.load(y_file_path), dtype=torch.float32)\n",
    "\n",
    "# print(torch.unique(y_raw))\n",
    "\n",
    "row_count = x_raw.shape[0]\n",
    "print(f'Row Count: {row_count}')\n",
    "\n",
    "x_train, x_val = torch.tensor_split(x_raw, [round(row_count * 0.8)], dim=0)\n",
    "y_train, y_val = torch.tensor_split(y_raw, [round(row_count * 0.8)], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train.to(device), y_train.to(device))\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val.to(device), y_val.to(device))\n",
    "\n",
    "loaders = {\n",
    "            'train' : torch.utils.data.DataLoader(train_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True, \n",
    "                                                num_workers=1),\n",
    "            \n",
    "            'validation'  : torch.utils.data.DataLoader(val_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False, \n",
    "                                                num_workers=1),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_struct = [n, 15, 12, 4, 1]\n",
    "\n",
    "class kryptonite_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(model_struct[0], model_struct[1])\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(model_struct[1], model_struct[2])\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(model_struct[2], model_struct[3])\n",
    "        self.act3= nn.ReLU()\n",
    "        self.output = nn.Linear(model_struct[3], model_struct[4])\n",
    "        self.act_output = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    " \n",
    "model = kryptonite_nn()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, epoch loss 0.6932265010351936\n",
      "Finished epoch 1, epoch loss 0.6928451371689638\n",
      "Finished epoch 2, epoch loss 0.6850147042423487\n",
      "Finished epoch 3, epoch loss 0.6594918090850115\n",
      "Finished epoch 4, epoch loss 0.603953769740959\n",
      "Finished epoch 5, epoch loss 0.5494265208641688\n",
      "Finished epoch 6, epoch loss 0.5054631175783774\n",
      "Finished epoch 7, epoch loss 0.475312734177957\n",
      "Finished epoch 8, epoch loss 0.4487015134654939\n",
      "Finished epoch 9, epoch loss 0.423111061565578\n",
      "Finished epoch 10, epoch loss 0.40084678772836924\n",
      "Finished epoch 11, epoch loss 0.3835740591554592\n",
      "Finished epoch 12, epoch loss 0.37097128418584663\n",
      "Finished epoch 13, epoch loss 0.36011227313429117\n",
      "Finished epoch 14, epoch loss 0.3377916032914072\n",
      "Finished epoch 15, epoch loss 0.3265215326100588\n",
      "Finished epoch 16, epoch loss 0.31986322975717485\n",
      "Finished epoch 17, epoch loss 0.31424223026260734\n",
      "Finished epoch 18, epoch loss 0.31001006431567174\n",
      "Finished epoch 19, epoch loss 0.30660335773912567\n",
      "Finished epoch 20, epoch loss 0.30271898698993027\n",
      "Finished epoch 21, epoch loss 0.30037615409431356\n",
      "Finished epoch 22, epoch loss 0.2982217059470713\n",
      "Finished epoch 23, epoch loss 0.2959252535365522\n",
      "Finished epoch 24, epoch loss 0.2956420749736329\n",
      "Finished epoch 25, epoch loss 0.29342936010410386\n",
      "Finished epoch 26, epoch loss 0.29222675738856196\n",
      "Finished epoch 27, epoch loss 0.29046281178792316\n",
      "Finished epoch 28, epoch loss 0.2897417906206101\n",
      "Finished epoch 29, epoch loss 0.28902958302448195\n"
     ]
    }
   ],
   "source": [
    "loss_func = 'BCE'           # Just used for recording in csv file\n",
    "optim_func = 'Adagrad'     # Just used for recording in csv file\n",
    "lr = 0.07\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 50\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    loss_arr = []\n",
    "    for data in loaders['train']:\n",
    "        x_vals, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        label_pred = model(x_vals)\n",
    "        labels = torch.reshape(labels, (batch_size, 1))\n",
    "        loss = loss_fn(label_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_arr.append(loss.item())\n",
    "\n",
    "    print(f'Finished epoch {epoch}, epoch loss {np.average(loss_arr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8965625\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data in loaders['train']:\n",
    "        x_vals, labels = data\n",
    "        output = model.forward(x_vals)\n",
    "\n",
    "        train_accuracy = np.concatenate((train_accuracy, torch.eq(torch.flatten(output.round()), labels).numpy() ))\n",
    "\n",
    "train_accuracy = train_accuracy.mean()\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.89125\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data in loaders['validation']:\n",
    "        x_vals, labels = data\n",
    "        output = model.forward(x_vals)\n",
    "        # y_pred = np.concatenate((y_pred, output.numpy().flatten()))\n",
    "        y_pred = np.concatenate((y_pred, torch.eq(torch.flatten(output.round()), labels).numpy() ))\n",
    "\n",
    "val_accuracy = y_pred.mean()        \n",
    "# val_accuracy = (y_pred.round() == y_val).float().mean()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model params to csv file\n",
    "\n",
    "csv_file_path = 'nn_model_record.csv'\n",
    "\n",
    "fields = ['n', 'train_acc', 'val_acc', 'optim_func', 'learning_rate', 'loss_func', 'epochs', 'batch_size', 'model_struct']\n",
    "\n",
    "if not os.path.isfile(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(fields)\n",
    "\n",
    "with open(csv_file_path, mode='a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([n, round(train_accuracy.item(), 3), round(val_accuracy.item(), 3), optim_func, lr, loss_func, n_epochs, batch_size, model_struct])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
