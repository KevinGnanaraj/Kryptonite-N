{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ' + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n = 24\n",
    "\n",
    "x_file_path = 'Datasets/kryptonite-' + str(n) + '-X.npy'\n",
    "y_file_path = 'Datasets/kryptonite-' + str(n) + '-y.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Count: 48000\n"
     ]
    }
   ],
   "source": [
    "x_raw = torch.tensor(np.load(x_file_path), dtype=torch.float32)\n",
    "y_raw = torch.tensor(np.load(y_file_path), dtype=torch.float32)\n",
    "\n",
    "# print(torch.unique(y_raw))\n",
    "\n",
    "row_count = x_raw.shape[0]\n",
    "print(f'Row Count: {row_count}')\n",
    "\n",
    "x_train, x_val = torch.tensor_split(x_raw, [round(row_count * 0.8)], dim=0)\n",
    "y_train, y_val = torch.tensor_split(y_raw, [round(row_count * 0.8)], dim=0)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train.to(device), y_train.to(device))\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val.to(device), y_val.to(device))\n",
    "\n",
    "loaders = {\n",
    "            'train' : torch.utils.data.DataLoader(train_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True, \n",
    "                                                num_workers=1),\n",
    "            \n",
    "            'validation'  : torch.utils.data.DataLoader(val_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False, \n",
    "                                                num_workers=1),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_struct = [n, 15, 10, 4, 1]\n",
    "dropout = [0.0, 0.0, 0.0]\n",
    "\n",
    "class kryptonite_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(model_struct[0], model_struct[1])\n",
    "        self.act1 = nn.ReLU()\n",
    "        # self.dropout1 = nn.Dropout(dropout[0])\n",
    "        self.layer2 = nn.Linear(model_struct[1], model_struct[2])\n",
    "        self.act2 = nn.ReLU()\n",
    "        # self.dropout2 = nn.Dropout(dropout[1])\n",
    "        self.layer3 = nn.Linear(model_struct[2], model_struct[3])\n",
    "        self.act3= nn.ReLU()\n",
    "        # self.dropout3 = nn.Dropout(dropout[2])\n",
    "        self.output = nn.Linear(model_struct[3], model_struct[4])\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "        # nn.init.xavier_normal_(self.layer1.weight)\n",
    "        # nn.init.zeros_(self.layer1.bias)\n",
    "\n",
    "        # nn.init.xavier_normal_(self.layer2.weight)\n",
    "        # nn.init.zeros_(self.layer2.bias)\n",
    "\n",
    "        # nn.init.xavier_normal_(self.layer3.weight)\n",
    "        # nn.init.zeros_(self.layer3.bias)\n",
    "\n",
    "        # nn.init.xavier_normal_(self.output.weight)\n",
    "        # nn.init.zeros_(self.output.bias)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        # x = self.dropout1(x)\n",
    "        x = self.act2(self.layer2(x))\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.act3(self.layer3(x))\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.act4(self.layer4(x))\n",
    "        # x = self.dropout4(x)\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    " \n",
    "model = kryptonite_nn()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, epoch loss 0.7014455627774199\n",
      "Finished epoch 1, epoch loss 0.7028805643568437\n",
      "Finished epoch 2, epoch loss 0.7045880694997807\n",
      "Finished epoch 3, epoch loss 0.7043861852337917\n",
      "Finished epoch 4, epoch loss 0.7016813463717699\n",
      "Finished epoch 5, epoch loss 0.703259542932113\n",
      "Finished epoch 6, epoch loss 0.7038703122610848\n",
      "Finished epoch 7, epoch loss 0.7022624669099847\n",
      "Finished epoch 8, epoch loss 0.7023105363547802\n",
      "Finished epoch 9, epoch loss 0.7055048018383483\n",
      "Finished epoch 10, epoch loss 0.7035311099141837\n",
      "Finished epoch 11, epoch loss 0.7024109574283163\n",
      "Finished epoch 12, epoch loss 0.7035346231112878\n",
      "Finished epoch 13, epoch loss 0.7018485374189913\n",
      "Finished epoch 14, epoch loss 0.7034944099125763\n",
      "Finished epoch 15, epoch loss 0.702450593225658\n",
      "Finished epoch 16, epoch loss 0.703908827342093\n",
      "Finished epoch 17, epoch loss 0.7048097070554892\n",
      "Finished epoch 18, epoch loss 0.7028196165772775\n",
      "Finished epoch 19, epoch loss 0.7028680673489968\n",
      "Finished epoch 20, epoch loss 0.7030545642226934\n",
      "Finished epoch 21, epoch loss 0.7025761035953959\n",
      "Finished epoch 22, epoch loss 0.7024458446291586\n",
      "Finished epoch 23, epoch loss 0.7035600335523486\n",
      "Finished epoch 24, epoch loss 0.705685269224147\n",
      "Finished epoch 25, epoch loss 0.7043491607035199\n",
      "Finished epoch 26, epoch loss 0.7030402480438351\n",
      "Finished epoch 27, epoch loss 0.7029416395227114\n",
      "Finished epoch 28, epoch loss 0.7029512499707441\n",
      "Finished epoch 29, epoch loss 0.70569484492143\n",
      "Finished epoch 30, epoch loss 0.7031763711820046\n",
      "Finished epoch 31, epoch loss 0.7031557724003991\n",
      "Finished epoch 32, epoch loss 0.7028338924484948\n",
      "Finished epoch 33, epoch loss 0.7043759475027521\n",
      "Finished epoch 34, epoch loss 0.7023420454934239\n",
      "Finished epoch 35, epoch loss 0.7027357254674037\n",
      "Finished epoch 36, epoch loss 0.7018933532076578\n",
      "Finished epoch 37, epoch loss 0.703846920610716\n",
      "Finished epoch 38, epoch loss 0.7030122253174583\n",
      "Finished epoch 39, epoch loss 0.7025346234378715\n",
      "Finished epoch 40, epoch loss 0.7022822785253326\n",
      "Finished epoch 41, epoch loss 0.703531964123249\n",
      "Finished epoch 42, epoch loss 0.7031742837280035\n",
      "Finished epoch 43, epoch loss 0.7037313050900896\n",
      "Finished epoch 44, epoch loss 0.7054351303229729\n",
      "Finished epoch 45, epoch loss 0.7032390989921987\n",
      "Finished epoch 46, epoch loss 0.7025226458907128\n",
      "Finished epoch 47, epoch loss 0.703858613781631\n",
      "Finished epoch 48, epoch loss 0.708848038079838\n",
      "Finished epoch 49, epoch loss 0.7047552436466018\n",
      "Finished epoch 50, epoch loss 0.7057805209855239\n",
      "Finished epoch 51, epoch loss 0.7055562573050459\n",
      "Finished epoch 52, epoch loss 0.7073184795553485\n",
      "Finished epoch 53, epoch loss 0.7053341748937965\n",
      "Finished epoch 54, epoch loss 0.7067713305975\n",
      "Finished epoch 55, epoch loss 0.7050192249690493\n",
      "Finished epoch 56, epoch loss 0.7072643886506558\n",
      "Finished epoch 57, epoch loss 0.7073410766385496\n",
      "Finished epoch 58, epoch loss 0.7058652368560433\n",
      "Finished epoch 59, epoch loss 0.7084894141554833\n",
      "Finished epoch 60, epoch loss 0.707954511269927\n",
      "Finished epoch 61, epoch loss 0.7072753237870832\n",
      "Finished epoch 62, epoch loss 0.706655577843388\n",
      "Finished epoch 63, epoch loss 0.7048771435394883\n",
      "Finished epoch 64, epoch loss 0.7042707860469818\n",
      "Finished epoch 65, epoch loss 0.7044869318356116\n",
      "Finished epoch 66, epoch loss 0.7089819973893463\n",
      "Finished epoch 67, epoch loss 0.7098275545487801\n",
      "Finished epoch 68, epoch loss 0.7073750967035691\n",
      "Finished epoch 69, epoch loss 0.7057777100304763\n",
      "Finished epoch 70, epoch loss 0.708435964298745\n",
      "Finished epoch 71, epoch loss 0.7063006095526119\n",
      "Finished epoch 72, epoch loss 0.7047416098415852\n",
      "Finished epoch 73, epoch loss 0.7089827313895026\n",
      "Finished epoch 74, epoch loss 0.7072854270910224\n",
      "Finished epoch 75, epoch loss 0.7088836636021734\n",
      "Finished epoch 76, epoch loss 0.7073868168331683\n",
      "Finished epoch 77, epoch loss 0.7067236329987645\n",
      "Finished epoch 78, epoch loss 0.7059326494112611\n",
      "Finished epoch 79, epoch loss 0.70867368410031\n",
      "Finished epoch 80, epoch loss 0.7084194618215164\n",
      "Finished epoch 81, epoch loss 0.7062950012211998\n",
      "Finished epoch 82, epoch loss 0.7095401621609926\n",
      "Finished epoch 83, epoch loss 0.7062163923308253\n",
      "Finished epoch 84, epoch loss 0.7047620961318414\n",
      "Finished epoch 85, epoch loss 0.704432817051808\n",
      "Finished epoch 86, epoch loss 0.7036341094846527\n",
      "Finished epoch 87, epoch loss 0.7096330015237132\n",
      "Finished epoch 88, epoch loss 0.7070526524508993\n",
      "Finished epoch 89, epoch loss 0.7050009568408132\n",
      "Finished epoch 90, epoch loss 0.7061090689276656\n",
      "Finished epoch 91, epoch loss 0.7064719757686059\n",
      "Finished epoch 92, epoch loss 0.7076906133256853\n",
      "Finished epoch 93, epoch loss 0.704859667258958\n",
      "Finished epoch 94, epoch loss 0.7042194671804706\n",
      "Finished epoch 95, epoch loss 0.7042207349402209\n",
      "Finished epoch 96, epoch loss 0.7049710501606266\n",
      "Finished epoch 97, epoch loss 0.7032789378985762\n",
      "Finished epoch 98, epoch loss 0.7046937287971378\n",
      "Finished epoch 99, epoch loss 0.7080843281062941\n",
      "Finished epoch 100, epoch loss 0.7082087856096526\n",
      "Finished epoch 101, epoch loss 0.7108713310273985\n",
      "Finished epoch 102, epoch loss 0.70925744642814\n",
      "Finished epoch 103, epoch loss 0.7032752567032973\n",
      "Finished epoch 104, epoch loss 0.7105603428309163\n",
      "Finished epoch 105, epoch loss 0.7057705708344777\n",
      "Finished epoch 106, epoch loss 0.7056719988957048\n",
      "Finished epoch 107, epoch loss 0.7012609410534303\n",
      "Finished epoch 108, epoch loss 0.7030623963226875\n",
      "Finished epoch 109, epoch loss 0.708823967948556\n",
      "Finished epoch 110, epoch loss 0.7037023043011625\n",
      "Finished epoch 111, epoch loss 0.7016850573507448\n",
      "Finished epoch 112, epoch loss 0.7106691824148098\n",
      "Finished epoch 113, epoch loss 0.7083584285279115\n",
      "Finished epoch 114, epoch loss 0.7035557068511844\n",
      "Finished epoch 115, epoch loss 0.7041627970462044\n",
      "Finished epoch 116, epoch loss 0.7086718929496905\n",
      "Finished epoch 117, epoch loss 0.7014593481893341\n",
      "Finished epoch 118, epoch loss 0.7007530585676431\n",
      "Finished epoch 119, epoch loss 0.7058416625609001\n",
      "Finished epoch 120, epoch loss 0.7058879961321751\n",
      "Finished epoch 121, epoch loss 0.7067169692181051\n",
      "Finished epoch 122, epoch loss 0.7053140687321623\n",
      "Finished epoch 123, epoch loss 0.7125058851018548\n",
      "Finished epoch 124, epoch loss 0.7045254768120746\n",
      "Finished epoch 125, epoch loss 0.7009001966938376\n",
      "Finished epoch 126, epoch loss 0.7038481464609504\n",
      "Finished epoch 127, epoch loss 0.703091138266027\n",
      "Finished epoch 128, epoch loss 0.7080043042326967\n",
      "Finished epoch 129, epoch loss 0.7057267545598248\n",
      "Finished epoch 130, epoch loss 0.7049449285119772\n",
      "Finished epoch 131, epoch loss 0.7040000151842832\n",
      "Finished epoch 132, epoch loss 0.7022245073318482\n",
      "Finished epoch 133, epoch loss 0.7041105218728383\n",
      "Finished epoch 134, epoch loss 0.7045434526726604\n",
      "Finished epoch 135, epoch loss 0.7054063863555591\n",
      "Finished epoch 136, epoch loss 0.7022153544674317\n",
      "Finished epoch 137, epoch loss 0.7048652451485395\n",
      "Finished epoch 138, epoch loss 0.7069244974168638\n",
      "Finished epoch 139, epoch loss 0.7018072991818189\n",
      "Finished epoch 140, epoch loss 0.702462455444038\n",
      "Finished epoch 141, epoch loss 0.711261280272156\n",
      "Finished epoch 142, epoch loss 0.7036229236610234\n",
      "Finished epoch 143, epoch loss 0.7155751292655865\n",
      "Finished epoch 144, epoch loss 0.7065397451942166\n",
      "Finished epoch 145, epoch loss 0.7070560544480881\n",
      "Finished epoch 146, epoch loss 0.7070483212297162\n",
      "Finished epoch 147, epoch loss 0.7042652772677441\n",
      "Finished epoch 148, epoch loss 0.7085917970910668\n",
      "Finished epoch 149, epoch loss 0.707621990647167\n"
     ]
    }
   ],
   "source": [
    "loss_func = 'BCE'           # Just used for recording in csv file\n",
    "optim_func = 'Adadelta'     # Just used for recording in csv file\n",
    "lr = 0.5\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 150\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    loss_arr = []\n",
    "    for data in loaders['train']:\n",
    "        x_vals, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        label_pred = model(x_vals)\n",
    "        labels = torch.reshape(labels, (batch_size, 1))\n",
    "        loss = loss_fn(label_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_arr.append(loss.item())\n",
    "\n",
    "    print(f'Finished epoch {epoch}, epoch loss {np.average(loss_arr)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.554375\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data in loaders['train']:\n",
    "        x_vals, labels = data\n",
    "        output = model.forward(x_vals)\n",
    "\n",
    "        train_accuracy = np.concatenate((train_accuracy, torch.eq(torch.flatten(output.round()), labels).numpy() ))\n",
    "\n",
    "train_accuracy = train_accuracy.mean()\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4990625\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data in loaders['validation']:\n",
    "        x_vals, labels = data\n",
    "        output = model.forward(x_vals)\n",
    "        # y_pred = np.concatenate((y_pred, output.numpy().flatten()))\n",
    "        y_pred = np.concatenate((y_pred, torch.eq(torch.flatten(output.round()), labels).numpy() ))\n",
    "\n",
    "val_accuracy = y_pred.mean()        \n",
    "# val_accuracy = (y_pred.round() == y_val).float().mean()\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model params to csv file\n",
    "\n",
    "csv_file_path = 'nn_model_record.csv'\n",
    "\n",
    "fields = ['n', 'train_acc', 'val_acc', 'optim_func', 'learning_rate', 'loss_func', 'epochs', 'batch_size', 'model_struct', 'dropout']\n",
    "\n",
    "if not os.path.isfile(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(fields)\n",
    "\n",
    "with open(csv_file_path, mode='a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([n, round(train_accuracy.item(), 3), round(val_accuracy.item(), 3), optim_func, lr, loss_func, n_epochs, batch_size, model_struct, dropout])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Saved_Models/n-' + str(n) + '-' + str(round(val_accuracy.item(), 3))[2:] + '.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
